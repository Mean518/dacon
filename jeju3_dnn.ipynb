{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset_min.csv',index_col=0)\n",
    "submission = pd.read_csv('submission_min.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>사용년</th>\n",
       "      <th>사용월</th>\n",
       "      <th>사용지역_도1</th>\n",
       "      <th>업종명_대분류1</th>\n",
       "      <th>업종명1</th>\n",
       "      <th>이용금액</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1394 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      사용년  사용월  사용지역_도1  업종명_대분류1  업종명1  이용금액\n",
       "0       1    4        4         1    31     0\n",
       "1       1    4        4         3    23     0\n",
       "2       1    4        4         1    13     0\n",
       "3       1    4        4         1    32     0\n",
       "4       1    4        4        18    40     0\n",
       "...   ...  ...      ...       ...   ...   ...\n",
       "1389    1    7        8         0     7     0\n",
       "1390    1    7        8         0     0     0\n",
       "1391    1    7        8        14    27     0\n",
       "1392    1    7        8         1    15     0\n",
       "1393    1    7        8         3    30     0\n",
       "\n",
       "[1394 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[0,4] = 333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>사용년</th>\n",
       "      <th>사용월</th>\n",
       "      <th>사용지역_도1</th>\n",
       "      <th>업종명_대분류1</th>\n",
       "      <th>업종명1</th>\n",
       "      <th>이용금액</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1394 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      사용년  사용월  사용지역_도1  업종명_대분류1  업종명1  이용금액\n",
       "0       1    4        4         1   333     0\n",
       "1       1    4        4         3    23     0\n",
       "2       1    4        4         1    13     0\n",
       "3       1    4        4         1    32     0\n",
       "4       1    4        4        18    40     0\n",
       "...   ...  ...      ...       ...   ...   ...\n",
       "1389    1    7        8         0     7     0\n",
       "1390    1    7        8         0     0     0\n",
       "1391    1    7        8        14    27     0\n",
       "1392    1    7        8         1    15     0\n",
       "1393    1    7        8         3    30     0\n",
       "\n",
       "[1394 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>사용년</th>\n",
       "      <th>사용월</th>\n",
       "      <th>사용지역_도1</th>\n",
       "      <th>업종명_대분류1</th>\n",
       "      <th>업종명1</th>\n",
       "      <th>이용금액</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [사용년, 사용월, 사용지역_도1, 업종명_대분류1, 업종명1, 이용금액]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset['이용금액'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cor = dataset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>사용년</th>\n",
       "      <th>사용월</th>\n",
       "      <th>사용지역_도1</th>\n",
       "      <th>업종명_대분류1</th>\n",
       "      <th>업종명1</th>\n",
       "      <th>이용금액</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>사용년</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.499500</td>\n",
       "      <td>-0.002615</td>\n",
       "      <td>-0.005513</td>\n",
       "      <td>-0.008526</td>\n",
       "      <td>-0.013327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>사용월</th>\n",
       "      <td>-0.499500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.011533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>사용지역_도1</th>\n",
       "      <td>-0.002615</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.028137</td>\n",
       "      <td>-0.044994</td>\n",
       "      <td>-0.256963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>업종명_대분류1</th>\n",
       "      <td>-0.005513</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>-0.028137</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.589526</td>\n",
       "      <td>-0.187674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>업종명1</th>\n",
       "      <td>-0.008526</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>-0.044994</td>\n",
       "      <td>0.589526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.370045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>이용금액</th>\n",
       "      <td>-0.013327</td>\n",
       "      <td>0.011533</td>\n",
       "      <td>-0.256963</td>\n",
       "      <td>-0.187674</td>\n",
       "      <td>-0.370045</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               사용년       사용월   사용지역_도1  업종명_대분류1      업종명1      이용금액\n",
       "사용년       1.000000 -0.499500 -0.002615 -0.005513 -0.008526 -0.013327\n",
       "사용월      -0.499500  1.000000  0.001876  0.000256  0.002351  0.011533\n",
       "사용지역_도1  -0.002615  0.001876  1.000000 -0.028137 -0.044994 -0.256963\n",
       "업종명_대분류1 -0.005513  0.000256 -0.028137  1.000000  0.589526 -0.187674\n",
       "업종명1     -0.008526  0.002351 -0.044994  0.589526  1.000000 -0.370045\n",
       "이용금액     -0.013327  0.011533 -0.256963 -0.187674 -0.370045  1.000000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.iloc[:,:-1]\n",
    "y = dataset.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>사용년</th>\n",
       "      <th>사용월</th>\n",
       "      <th>사용지역_도1</th>\n",
       "      <th>업종명_대분류1</th>\n",
       "      <th>업종명1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9433</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9434</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9435</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9436</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9437</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9438 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      사용년  사용월  사용지역_도1  업종명_대분류1  업종명1\n",
       "0       0    1        0         0     0\n",
       "1       0    1        0         0     6\n",
       "2       0    1        0         0     7\n",
       "3       0    1        0         0     9\n",
       "4       0    1        0         0    11\n",
       "...   ...  ...      ...       ...   ...\n",
       "9433    1    3       16        11    21\n",
       "9434    1    3       16        12    26\n",
       "9435    1    3       16        12    33\n",
       "9436    1    3       16        13    24\n",
       "9437    1    3       16        17    37\n",
       "\n",
       "[9438 rows x 5 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = submission.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 머신러닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_squared_log_error\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline , make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, random_state=66, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"scaler\",RobustScaler()),('rf',RandomForestRegressor())]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    { 'rf__max_depth':[3,5,25,100, 200, 300],\n",
    "     'rf__n_estimators':[10, 50, 150, 250, 300],\n",
    "    'rf__max_leaf_nodes':[1000000,1005000,10000000]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmsle는 0.4644819596117981\n",
      "mae는 488800078.0807945\n"
     ]
    }
   ],
   "source": [
    "model = RandomizedSearchCV(pipe, parameters, cv=5)\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "RMSLE = np.sqrt(mean_squared_log_error(y_test, pred))\n",
    "MAE = mean_absolute_error(y_test, pred)\n",
    "print('rmsle는',RMSLE)\n",
    "print('mae는',MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test)\n",
    "real_submission = pd.read_csv('submission1.csv')\n",
    "real_submission['AMT'] = test_pred\n",
    "real_submission.to_csv('submission_0630_3.csv', index=None, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "최적의 매개 변수는(estimator): Pipeline(memory=None,\n",
      "         steps=[('scaler',\n",
      "                 RobustScaler(copy=True, quantile_range=(25.0, 75.0),\n",
      "                              with_centering=True, with_scaling=True)),\n",
      "                ('rf',\n",
      "                 RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,\n",
      "                                       criterion='mse', max_depth=100,\n",
      "                                       max_features='auto',\n",
      "                                       max_leaf_nodes=1005000, max_samples=None,\n",
      "                                       min_impurity_decrease=0.0,\n",
      "                                       min_impurity_split=None,\n",
      "                                       min_samples_leaf=1, min_samples_split=2,\n",
      "                                       min_weight_fraction_leaf=0.0,\n",
      "                                       n_estimators=300, n_jobs=None,\n",
      "                                       oob_score=False, random_state=None,\n",
      "                                       verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "============================\n",
      "최적의 매개 변수는(params): {'rf__n_estimators': 300, 'rf__max_leaf_nodes': 1005000, 'rf__max_depth': 100}\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "print(\"============================\")\n",
    "print(\"최적의 매개 변수는(estimator):\", model.best_estimator_) # 전체 params 다 나옴\n",
    "print(\"============================\")\n",
    "print(\"최적의 매개 변수는(params):\", model.best_params_)\n",
    "print(\"============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4715094243268884"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "RMSLE = np.sqrt(mean_squared_log_error(y_test, pred))\n",
    "RMSLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmsle는 0.4647577750664943\n",
      "mae는 492503686.91604173\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators=150)\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "RMSLE = np.sqrt(mean_squared_log_error(y_test, pred))\n",
    "MAE = mean_absolute_error(y_test, pred)\n",
    "print('rmsle는',RMSLE)\n",
    "print('mae는',MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mean Squared Logarithmic Error cannot be used when targets contain negative values.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-6135ed00ca51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mRMSLE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_log_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mMAE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rmsle는'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mRMSLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36mmean_squared_log_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m         raise ValueError(\"Mean Squared Logarithmic Error cannot be used when \"\n\u001b[0m\u001b[0;32m    327\u001b[0m                          \"targets contain negative values.\")\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Mean Squared Logarithmic Error cannot be used when targets contain negative values."
     ]
    }
   ],
   "source": [
    "model = XGBRegressor(n_estimators=150)\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "RMSLE = np.sqrt(mean_squared_log_error(y_test, pred))\n",
    "MAE = mean_absolute_error(y_test, pred)\n",
    "print('rmsle는',RMSLE)\n",
    "print('mae는',MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test)\n",
    "real_submission = pd.read_csv('submission1.csv')\n",
    "real_submission['AMT'] = test_pred\n",
    "real_submission.to_csv('submission_0630_2.csv', index=None, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>REG_YYMM</th>\n",
       "      <th>CARD_SIDO_NM</th>\n",
       "      <th>STD_CLSS_NM</th>\n",
       "      <th>AMT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>202004</td>\n",
       "      <td>강원</td>\n",
       "      <td>건강보조식품 소매업</td>\n",
       "      <td>9.751899e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>202004</td>\n",
       "      <td>강원</td>\n",
       "      <td>골프장 운영업</td>\n",
       "      <td>3.760515e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>202004</td>\n",
       "      <td>강원</td>\n",
       "      <td>과실 및 채소 소매업</td>\n",
       "      <td>9.334674e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>202004</td>\n",
       "      <td>강원</td>\n",
       "      <td>관광 민예품 및 선물용품 소매업</td>\n",
       "      <td>1.834808e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>202004</td>\n",
       "      <td>강원</td>\n",
       "      <td>그외 기타 분류안된 오락관련 서비스업</td>\n",
       "      <td>1.619220e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>1389</td>\n",
       "      <td>202007</td>\n",
       "      <td>충북</td>\n",
       "      <td>피자 햄버거 샌드위치 및 유사 음식점업</td>\n",
       "      <td>1.462200e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>1390</td>\n",
       "      <td>202007</td>\n",
       "      <td>충북</td>\n",
       "      <td>한식 음식점업</td>\n",
       "      <td>2.084806e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>1391</td>\n",
       "      <td>202007</td>\n",
       "      <td>충북</td>\n",
       "      <td>호텔업</td>\n",
       "      <td>1.708192e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>1392</td>\n",
       "      <td>202007</td>\n",
       "      <td>충북</td>\n",
       "      <td>화장품 및 방향제 소매업</td>\n",
       "      <td>4.778178e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>1393</td>\n",
       "      <td>202007</td>\n",
       "      <td>충북</td>\n",
       "      <td>휴양콘도 운영업</td>\n",
       "      <td>2.197401e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1394 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  REG_YYMM CARD_SIDO_NM            STD_CLSS_NM           AMT\n",
       "0        0    202004           강원             건강보조식품 소매업  9.751899e+07\n",
       "1        1    202004           강원                골프장 운영업  3.760515e+09\n",
       "2        2    202004           강원            과실 및 채소 소매업  9.334674e+08\n",
       "3        3    202004           강원      관광 민예품 및 선물용품 소매업  1.834808e+07\n",
       "4        4    202004           강원   그외 기타 분류안된 오락관련 서비스업  1.619220e+06\n",
       "...    ...       ...          ...                    ...           ...\n",
       "1389  1389    202007           충북  피자 햄버거 샌드위치 및 유사 음식점업  1.462200e+09\n",
       "1390  1390    202007           충북                한식 음식점업  2.084806e+10\n",
       "1391  1391    202007           충북                    호텔업  1.708192e+07\n",
       "1392  1392    202007           충북          화장품 및 방향제 소매업  4.778178e+08\n",
       "1393  1393    202007           충북               휴양콘도 운영업  2.197401e+08\n",
       "\n",
       "[1394 rows x 5 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_real.to_csv('submission_0630_1.csv', index=None, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>REG_YYMM</th>\n",
       "      <th>CARD_SIDO_NM</th>\n",
       "      <th>STD_CLSS_NM</th>\n",
       "      <th>AMT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>202004</td>\n",
       "      <td>강원</td>\n",
       "      <td>건강보조식품 소매업</td>\n",
       "      <td>9.751899e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>202004</td>\n",
       "      <td>강원</td>\n",
       "      <td>골프장 운영업</td>\n",
       "      <td>3.760515e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>202004</td>\n",
       "      <td>강원</td>\n",
       "      <td>과실 및 채소 소매업</td>\n",
       "      <td>9.334674e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>202004</td>\n",
       "      <td>강원</td>\n",
       "      <td>관광 민예품 및 선물용품 소매업</td>\n",
       "      <td>1.834808e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>202004</td>\n",
       "      <td>강원</td>\n",
       "      <td>그외 기타 분류안된 오락관련 서비스업</td>\n",
       "      <td>1.619220e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>1389</td>\n",
       "      <td>202007</td>\n",
       "      <td>충북</td>\n",
       "      <td>피자 햄버거 샌드위치 및 유사 음식점업</td>\n",
       "      <td>1.462200e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>1390</td>\n",
       "      <td>202007</td>\n",
       "      <td>충북</td>\n",
       "      <td>한식 음식점업</td>\n",
       "      <td>2.084806e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>1391</td>\n",
       "      <td>202007</td>\n",
       "      <td>충북</td>\n",
       "      <td>호텔업</td>\n",
       "      <td>1.708192e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>1392</td>\n",
       "      <td>202007</td>\n",
       "      <td>충북</td>\n",
       "      <td>화장품 및 방향제 소매업</td>\n",
       "      <td>4.778178e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>1393</td>\n",
       "      <td>202007</td>\n",
       "      <td>충북</td>\n",
       "      <td>휴양콘도 운영업</td>\n",
       "      <td>2.197401e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1394 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  REG_YYMM CARD_SIDO_NM            STD_CLSS_NM           AMT\n",
       "0        0    202004           강원             건강보조식품 소매업  9.751899e+07\n",
       "1        1    202004           강원                골프장 운영업  3.760515e+09\n",
       "2        2    202004           강원            과실 및 채소 소매업  9.334674e+08\n",
       "3        3    202004           강원      관광 민예품 및 선물용품 소매업  1.834808e+07\n",
       "4        4    202004           강원   그외 기타 분류안된 오락관련 서비스업  1.619220e+06\n",
       "...    ...       ...          ...                    ...           ...\n",
       "1389  1389    202007           충북  피자 햄버거 샌드위치 및 유사 음식점업  1.462200e+09\n",
       "1390  1390    202007           충북                한식 음식점업  2.084806e+10\n",
       "1391  1391    202007           충북                    호텔업  1.708192e+07\n",
       "1392  1392    202007           충북          화장품 및 방향제 소매업  4.778178e+08\n",
       "1393  1393    202007           충북               휴양콘도 운영업  2.197401e+08\n",
       "\n",
       "[1394 rows x 5 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_real = submission_real.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_real.to_csv('submission_0630_1.csv', index=None, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb 완전 쓰레기..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7550, 5)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1888, 5)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 1600)              9600      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 3600)              5763600   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 3600)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1280)              4609280   \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 640)               819840    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 641       \n",
      "=================================================================\n",
      "Total params: 11,202,961\n",
      "Trainable params: 11,202,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1600, input_shape=(5,), activation='elu'))\n",
    "model.add(Dense(3600, activation='elu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1280, activation='elu'))\n",
    "model.add(Dense(640, activation='elu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation='elu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6040 samples, validate on 1510 samples\n",
      "Epoch 1/500\n",
      "6040/6040 [==============================] - 1s 116us/step - loss: 402215335456531480576.0000 - mse: 402215321636180066304.0000 - val_loss: 369562660599200153600.0000 - val_mse: 369562641040999972864.0000\n",
      "Epoch 2/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 396847060079305621504.0000 - mse: 396846995695982346240.0000 - val_loss: 358979894240020332544.0000 - val_mse: 358979920788493172736.0000\n",
      "Epoch 3/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 386215249669018288128.0000 - mse: 386215298797527891968.0000 - val_loss: 350154815819465555968.0000 - val_mse: 350154800659311886336.0000\n",
      "Epoch 4/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 370406392953511149568.0000 - mse: 370406432652434341888.0000 - val_loss: 326204603076538662912.0000 - val_mse: 326204587472211410944.0000\n",
      "Epoch 5/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 331049644527809855488.0000 - mse: 331049651430704021504.0000 - val_loss: 285354500091481358336.0000 - val_mse: 285354493538100838400.0000\n",
      "Epoch 6/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 287076322725369479168.0000 - mse: 287076311155012009984.0000 - val_loss: 248240334659410919424.0000 - val_mse: 248240346601126625280.0000\n",
      "Epoch 7/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 253563100092599304192.0000 - mse: 253563038410725130240.0000 - val_loss: 223383174087733444608.0000 - val_mse: 223383168262506283008.0000\n",
      "Epoch 8/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 230503532719517958144.0000 - mse: 230503587971797614592.0000 - val_loss: 205966545186288697344.0000 - val_mse: 205966517050441465856.0000\n",
      "Epoch 9/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 214392770608541663232.0000 - mse: 214392716768879575040.0000 - val_loss: 193684243257508134912.0000 - val_mse: 193684268480741769216.0000\n",
      "Epoch 10/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 200547717781398552576.0000 - mse: 200547719128482316288.0000 - val_loss: 182665578557177692160.0000 - val_mse: 182665596265868296192.0000\n",
      "Epoch 11/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 188486708361355067392.0000 - mse: 188486674706105106432.0000 - val_loss: 171950712210855133184.0000 - val_mse: 171950705919609798656.0000\n",
      "Epoch 12/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 177579056952034656256.0000 - mse: 177579044369543987200.0000 - val_loss: 161780309925557501952.0000 - val_mse: 161780328915798065152.0000\n",
      "Epoch 13/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 167436812631613177856.0000 - mse: 167436814863403319296.0000 - val_loss: 153351854446151237632.0000 - val_mse: 153351859805360226304.0000\n",
      "Epoch 14/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 159222702163714768896.0000 - mse: 159222688947730644992.0000 - val_loss: 143905252859614117888.0000 - val_mse: 143905269315880878080.0000\n",
      "Epoch 15/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 151455959046397100032.0000 - mse: 151455967507539558400.0000 - val_loss: 135635231297047756800.0000 - val_mse: 135635226636866027520.0000\n",
      "Epoch 16/500\n",
      "6040/6040 [==============================] - 1s 91us/step - loss: 147395114169057329152.0000 - mse: 147395084038069813248.0000 - val_loss: 131312183420673769472.0000 - val_mse: 131312166818776350720.0000\n",
      "Epoch 17/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 140466843702859317248.0000 - mse: 140466841369127682048.0000 - val_loss: 126112254865862426624.0000 - val_mse: 126112250875581825024.0000\n",
      "Epoch 18/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 137515567336641183744.0000 - mse: 137515558646130409472.0000 - val_loss: 122336278888211513344.0000 - val_mse: 122336272859101396992.0000\n",
      "Epoch 19/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 135596353865643409408.0000 - mse: 135596374293986934784.0000 - val_loss: 119499574043795390464.0000 - val_mse: 119499576839904428032.0000\n",
      "Epoch 20/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 132334317365459369984.0000 - mse: 132334316808422031360.0000 - val_loss: 116257576281376718848.0000 - val_mse: 116257583242523181056.0000\n",
      "Epoch 21/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 130990610405380489216.0000 - mse: 130990634434442559488.0000 - val_loss: 113502940353786167296.0000 - val_mse: 113502937179037368320.0000\n",
      "Epoch 22/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 129860761613449019392.0000 - mse: 129860749897460875264.0000 - val_loss: 110075991162796277760.0000 - val_mse: 110075979337585131520.0000\n",
      "Epoch 23/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 127475587635197427712.0000 - mse: 127475583721372909568.0000 - val_loss: 110390730254586298368.0000 - val_mse: 110390721138105778176.0000\n",
      "Epoch 24/500\n",
      "6040/6040 [==============================] - 1s 91us/step - loss: 125915857535809126400.0000 - mse: 125915860506674987008.0000 - val_loss: 108858827363675602944.0000 - val_mse: 108858811169544077312.0000\n",
      "Epoch 25/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 126011234337010008064.0000 - mse: 126011245339407810560.0000 - val_loss: 108946451377321410560.0000 - val_mse: 108946455440417357824.0000\n",
      "Epoch 26/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 125081875665176297472.0000 - mse: 125081894131146424320.0000 - val_loss: 105662406790260588544.0000 - val_mse: 105662416518389956608.0000\n",
      "Epoch 27/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 122756529405845340160.0000 - mse: 122756532591516450816.0000 - val_loss: 104016178864753934336.0000 - val_mse: 104016183728818618368.0000\n",
      "Epoch 28/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 122167972409135841280.0000 - mse: 122167959619121446912.0000 - val_loss: 101912024239429353472.0000 - val_mse: 101912017540418109440.0000\n",
      "Epoch 29/500\n",
      "6040/6040 [==============================] - 1s 85us/step - loss: 121655426238628904960.0000 - mse: 121655428870903431168.0000 - val_loss: 100426053250167586816.0000 - val_mse: 100426049565711400960.0000\n",
      "Epoch 30/500\n",
      "6040/6040 [==============================] - 1s 85us/step - loss: 120691581508376084480.0000 - mse: 120691588181901967360.0000 - val_loss: 101217436763650080768.0000 - val_mse: 101217451647105499136.0000\n",
      "Epoch 31/500\n",
      "6040/6040 [==============================] - 1s 85us/step - loss: 119298394821168332800.0000 - mse: 119298401396393508864.0000 - val_loss: 99578876846798618624.0000 - val_mse: 99578880254556504064.0000\n",
      "Epoch 32/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 119330110037470117888.0000 - mse: 119330084923459502080.0000 - val_loss: 99917582321208279040.0000 - val_mse: 99917582612469645312.0000\n",
      "Epoch 33/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 118545522831086452736.0000 - mse: 118545517406343659520.0000 - val_loss: 98418847574759374848.0000 - val_mse: 98418851506787713024.0000\n",
      "Epoch 34/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 117345103760321167360.0000 - mse: 117345114591602933760.0000 - val_loss: 96323375377711906816.0000 - val_mse: 96323367062200123392.0000\n",
      "Epoch 35/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 116666182351942008832.0000 - mse: 116666170559497764864.0000 - val_loss: 98685647231424954368.0000 - val_mse: 98685645804244303872.0000\n",
      "Epoch 36/500\n",
      "6040/6040 [==============================] - 1s 92us/step - loss: 116680047342839726080.0000 - mse: 116680050794286809088.0000 - val_loss: 97303081902997635072.0000 - val_mse: 97303075903013650432.0000\n",
      "Epoch 37/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 115410503141081579520.0000 - mse: 115410519484484550656.0000 - val_loss: 94727864544962805760.0000 - val_mse: 94727878933273903104.0000\n",
      "Epoch 38/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 114290338407034732544.0000 - mse: 114290337038106361856.0000 - val_loss: 94769444681530458112.0000 - val_mse: 94769431676710813696.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 114655107313077305344.0000 - mse: 114655111015737327616.0000 - val_loss: 95393270909325557760.0000 - val_mse: 95393268186031849472.0000\n",
      "Epoch 40/500\n",
      "6040/6040 [==============================] - 1s 92us/step - loss: 113993319379588530176.0000 - mse: 113993328161118486528.0000 - val_loss: 96313052419833872384.0000 - val_mse: 96313049245085073408.0000\n",
      "Epoch 41/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 114248800248510349312.0000 - mse: 114248810682948517888.0000 - val_loss: 94561087941657247744.0000 - val_mse: 94561096213479817216.0000\n",
      "Epoch 42/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 113198707030065758208.0000 - mse: 113198689117492215808.0000 - val_loss: 91523588812906807296.0000 - val_mse: 91523594186678861824.0000\n",
      "Epoch 43/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 112371443885428195328.0000 - mse: 112371451753125642240.0000 - val_loss: 92332237556995899392.0000 - val_mse: 92332245406489509888.0000\n",
      "Epoch 44/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 111120467711769493504.0000 - mse: 111120453811321176064.0000 - val_loss: 91576841685302427648.0000 - val_mse: 91576836937742286848.0000\n",
      "Epoch 45/500\n",
      "6040/6040 [==============================] - 1s 90us/step - loss: 111480649611853971456.0000 - mse: 111480627432301527040.0000 - val_loss: 90770872822408445952.0000 - val_mse: 90770877322396434432.0000\n",
      "Epoch 46/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 111466495678766465024.0000 - mse: 111466500906907860992.0000 - val_loss: 90542412903194329088.0000 - val_mse: 90542416398330626048.0000\n",
      "Epoch 47/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 110045573159050772480.0000 - mse: 110045580040100380672.0000 - val_loss: 89046419989231452160.0000 - val_mse: 89046420877578600448.0000\n",
      "Epoch 48/500\n",
      "6040/6040 [==============================] - 1s 90us/step - loss: 111055478375560708096.0000 - mse: 111055477072166125568.0000 - val_loss: 88071598659641737216.0000 - val_mse: 88071593868392398848.0000\n",
      "Epoch 49/500\n",
      "6040/6040 [==============================] - 1s 94us/step - loss: 108501889255902167040.0000 - mse: 108501865714702876672.0000 - val_loss: 92176152590195769344.0000 - val_mse: 92176149939717406720.0000\n",
      "Epoch 50/500\n",
      "6040/6040 [==============================] - 1s 94us/step - loss: 108872943126962061312.0000 - mse: 108872928898844721152.0000 - val_loss: 91322474927483961344.0000 - val_mse: 91322462723633053696.0000\n",
      "Epoch 51/500\n",
      "6040/6040 [==============================] - 1s 98us/step - loss: 108786761974755557376.0000 - mse: 108786779963785216000.0000 - val_loss: 90856668967147651072.0000 - val_mse: 90856674413735051264.0000\n",
      "Epoch 52/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 107951333015752966144.0000 - mse: 107951344640721944576.0000 - val_loss: 88099287799313170432.0000 - val_mse: 88099292765319331840.0000\n",
      "Epoch 53/500\n",
      "6040/6040 [==============================] - 1s 84us/step - loss: 108129672108821610496.0000 - mse: 108129685426747211776.0000 - val_loss: 87846891045944197120.0000 - val_mse: 87846888876047073280.0000\n",
      "Epoch 54/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 106101584734306238464.0000 - mse: 106101596646895779840.0000 - val_loss: 89839907254379200512.0000 - val_mse: 89839907633018961920.0000\n",
      "Epoch 55/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 106103176656027189248.0000 - mse: 106103162351453732864.0000 - val_loss: 88314216298341007360.0000 - val_mse: 88314225298316984320.0000\n",
      "Epoch 56/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 106476199457510326272.0000 - mse: 106476222248711618560.0000 - val_loss: 88024056883649871872.0000 - val_mse: 88024059781700386816.0000\n",
      "Epoch 57/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 105474036408438194176.0000 - mse: 105474039390226350080.0000 - val_loss: 87258821824631521280.0000 - val_mse: 87258826077047357440.0000\n",
      "Epoch 58/500\n",
      "6040/6040 [==============================] - 1s 95us/step - loss: 104676576862633000960.0000 - mse: 104676585596832972800.0000 - val_loss: 88164616207432302592.0000 - val_mse: 88164621348195270656.0000\n",
      "Epoch 59/500\n",
      "6040/6040 [==============================] - 1s 92us/step - loss: 105944845550806024192.0000 - mse: 105944832677053988864.0000 - val_loss: 87530140881853169664.0000 - val_mse: 87530132770224340992.0000\n",
      "Epoch 60/500\n",
      "6040/6040 [==============================] - 1s 91us/step - loss: 104182385083659911168.0000 - mse: 104182385906473238528.0000 - val_loss: 86439066585921224704.0000 - val_mse: 86439065391749660672.0000\n",
      "Epoch 61/500\n",
      "6040/6040 [==============================] - 1s 91us/step - loss: 105156619106565095424.0000 - mse: 105156614781333929984.0000 - val_loss: 86710739015985889280.0000 - val_mse: 86710732724740554752.0000\n",
      "Epoch 62/500\n",
      "6040/6040 [==============================] - 1s 84us/step - loss: 103611169955701915648.0000 - mse: 103611158829518028800.0000 - val_loss: 87204293329427791872.0000 - val_mse: 87204299096402690048.0000\n",
      "Epoch 63/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 103761284916595326976.0000 - mse: 103761272953035030528.0000 - val_loss: 87228829288196784128.0000 - val_mse: 87228831399841628160.0000\n",
      "Epoch 64/500\n",
      "6040/6040 [==============================] - 1s 84us/step - loss: 103479889665399980032.0000 - mse: 103479868345068552192.0000 - val_loss: 85901845801675653120.0000 - val_mse: 85901844010418307072.0000\n",
      "Epoch 65/500\n",
      "6040/6040 [==============================] - 1s 85us/step - loss: 102150802547358941184.0000 - mse: 102150805077691990016.0000 - val_loss: 86653056742793936896.0000 - val_mse: 86653056742793936896.0000\n",
      "Epoch 66/500\n",
      "6040/6040 [==============================] - 1s 84us/step - loss: 103005074452416348160.0000 - mse: 103005064039822786560.0000 - val_loss: 86940064243569541120.0000 - val_mse: 86940055665922539520.0000\n",
      "Epoch 67/500\n",
      "6040/6040 [==============================] - 1s 90us/step - loss: 101936804246090498048.0000 - mse: 101936796134461669376.0000 - val_loss: 87209608892906913792.0000 - val_mse: 87209603140495081472.0000\n",
      "Epoch 68/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 102206743103113314304.0000 - mse: 102206774617592299520.0000 - val_loss: 87411860707330686976.0000 - val_mse: 87411860503447732224.0000\n",
      "Epoch 69/500\n",
      "6040/6040 [==============================] - 1s 90us/step - loss: 101745936534770089984.0000 - mse: 101745929711972777984.0000 - val_loss: 85545792392996683776.0000 - val_mse: 85545786960972349440.0000\n",
      "Epoch 70/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 101399705613217857536.0000 - mse: 101399724286711693312.0000 - val_loss: 85523573097945890816.0000 - val_mse: 85523576826091274240.0000\n",
      "Epoch 71/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 101037119732900741120.0000 - mse: 101037114147964190720.0000 - val_loss: 86254044934236504064.0000 - val_mse: 86254039575027515392.0000\n",
      "Epoch 72/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 101503484274270109696.0000 - mse: 101503465407815614464.0000 - val_loss: 84893814080164134912.0000 - val_mse: 84893811750073270272.0000\n",
      "Epoch 73/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 100640277941608349696.0000 - mse: 100640278411267276800.0000 - val_loss: 86000104684988153856.0000 - val_mse: 86000105165569392640.0000\n",
      "Epoch 74/500\n",
      "6040/6040 [==============================] - 1s 94us/step - loss: 100394573054476238848.0000 - mse: 100394568348784918528.0000 - val_loss: 84310014124152979456.0000 - val_mse: 84310006260096303104.0000\n",
      "Epoch 75/500\n",
      "6040/6040 [==============================] - 1s 94us/step - loss: 100209240031656755200.0000 - mse: 100209243464900018176.0000 - val_loss: 84911499863046062080.0000 - val_mse: 84911500693140930560.0000\n",
      "Epoch 76/500\n",
      "6040/6040 [==============================] - 1s 91us/step - loss: 100300092138626383872.0000 - mse: 100300089513633382400.0000 - val_loss: 86970735506347655168.0000 - val_mse: 86970736438384001024.0000\n",
      "Epoch 77/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 98304143918601388032.0000 - mse: 98304150453778120704.0000 - val_loss: 85597411099852079104.0000 - val_mse: 85597411230919688192.0000\n",
      "Epoch 78/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 99204860796753526784.0000 - mse: 99204861583159197696.0000 - val_loss: 82671821240302223360.0000 - val_mse: 82671830691733307392.0000\n",
      "Epoch 79/500\n",
      "6040/6040 [==============================] - 1s 91us/step - loss: 98719524528119742464.0000 - mse: 98719519558472826880.0000 - val_loss: 84931978667597758464.0000 - val_mse: 84931969201603608576.0000\n",
      "Epoch 80/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 98239873444551147520.0000 - mse: 98239859809878802432.0000 - val_loss: 84992238137792954368.0000 - val_mse: 84992240030991777792.0000\n",
      "Epoch 81/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 98367737810720030720.0000 - mse: 98367737410235662336.0000 - val_loss: 83323116778258530304.0000 - val_mse: 83323111011283632128.0000\n",
      "Epoch 82/500\n",
      "6040/6040 [==============================] - 1s 90us/step - loss: 98888483818559422464.0000 - mse: 98888483709336420352.0000 - val_loss: 84810529853476225024.0000 - val_mse: 84810530341339004928.0000\n",
      "Epoch 83/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 98465581992469741568.0000 - mse: 98465593945107726336.0000 - val_loss: 82790807417794297856.0000 - val_mse: 82790797849858670592.0000\n",
      "Epoch 84/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 97956916584660697088.0000 - mse: 97956924681726459904.0000 - val_loss: 84130735928008933376.0000 - val_mse: 84130733088210681856.0000\n",
      "Epoch 85/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 96442711811550445568.0000 - mse: 96442730044511485952.0000 - val_loss: 83424366966692020224.0000 - val_mse: 83424362838062268416.0000\n",
      "Epoch 86/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 96205483024969596928.0000 - mse: 96205481823516491776.0000 - val_loss: 82473271365024628736.0000 - val_mse: 82473276483943006208.0000\n",
      "Epoch 87/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 95145391612343058432.0000 - mse: 95145394284666028032.0000 - val_loss: 82320549986332213248.0000 - val_mse: 82320549920798408704.0000\n",
      "Epoch 88/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 94616430727162822656.0000 - mse: 94616414842496483328.0000 - val_loss: 83195323279618883584.0000 - val_mse: 83195330167950016512.0000\n",
      "Epoch 89/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 94631568136993685504.0000 - mse: 94631552918587703296.0000 - val_loss: 81201354551882137600.0000 - val_mse: 81201352636838707200.0000\n",
      "Epoch 90/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 92068141592141496320.0000 - mse: 92068133917404692480.0000 - val_loss: 80466460363320066048.0000 - val_mse: 80466456657019273216.0000\n",
      "Epoch 91/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 93364072521999515648.0000 - mse: 93364079894552641536.0000 - val_loss: 83975853404600614912.0000 - val_mse: 83975851482275643392.0000\n",
      "Epoch 92/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 92543019582870142976.0000 - mse: 92543017387487657984.0000 - val_loss: 81943985552313778176.0000 - val_mse: 81943989178517684224.0000\n",
      "Epoch 93/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 91790779131150811136.0000 - mse: 91790766716135407616.0000 - val_loss: 80667598372464902144.0000 - val_mse: 80667596916158103552.0000\n",
      "Epoch 94/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 90300590903147675648.0000 - mse: 90300594208964083712.0000 - val_loss: 79855916789385084928.0000 - val_mse: 79855911044254793728.0000\n",
      "Epoch 95/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 92356045347805937664.0000 - mse: 92356047634207604736.0000 - val_loss: 77275158409735176192.0000 - val_mse: 77275154943725010944.0000\n",
      "Epoch 96/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 90560026165638791168.0000 - mse: 90560017380468064256.0000 - val_loss: 79456340414373691392.0000 - val_mse: 79456339722627973120.0000\n",
      "Epoch 97/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 89972475109204918272.0000 - mse: 89972473550956658688.0000 - val_loss: 77904953995380473856.0000 - val_mse: 77904964000208125952.0000\n",
      "Epoch 98/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 89681354521108922368.0000 - mse: 89681375648479707136.0000 - val_loss: 78293597307346485248.0000 - val_mse: 78293592982115319808.0000\n",
      "Epoch 99/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 88301220456821997568.0000 - mse: 88301233468923183104.0000 - val_loss: 80175669676042354688.0000 - val_mse: 80175666617798098944.0000\n",
      "Epoch 100/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 90095210843434483712.0000 - mse: 90095223029081571328.0000 - val_loss: 77709287960704942080.0000 - val_mse: 77709286114836086784.0000\n",
      "Epoch 101/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 88737241717547352064.0000 - mse: 88737229411754967040.0000 - val_loss: 79434656443136491520.0000 - val_mse: 79434648557235208192.0000\n",
      "Epoch 102/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 87930086590830018560.0000 - mse: 87930099916037160960.0000 - val_loss: 77166549248229244928.0000 - val_mse: 77166549583179808768.0000\n",
      "Epoch 103/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 86375979430313852928.0000 - mse: 86375971016501362688.0000 - val_loss: 78270784487181189120.0000 - val_mse: 78270775916815712256.0000\n",
      "Epoch 104/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 86519031845741510656.0000 - mse: 86519030673414553600.0000 - val_loss: 77158816499455639552.0000 - val_mse: 77158817817413287936.0000\n",
      "Epoch 105/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 86689413776800694272.0000 - mse: 86689428587440766976.0000 - val_loss: 77167705839801729024.0000 - val_mse: 77167701871365718016.0000\n",
      "Epoch 106/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 84557102009413320704.0000 - mse: 84557106105276170240.0000 - val_loss: 76815985715670827008.0000 - val_mse: 76815981295779708928.0000\n",
      "Epoch 107/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 85529753569310162944.0000 - mse: 85529751683392864256.0000 - val_loss: 74881403917376126976.0000 - val_mse: 74881394984754348032.0000\n",
      "Epoch 108/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 83609203201396015104.0000 - mse: 83609203936830947328.0000 - val_loss: 76221300383380193280.0000 - val_mse: 76221303834827292672.0000\n",
      "Epoch 109/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 83399172607981715456.0000 - mse: 83399153235460620288.0000 - val_loss: 74272429719712612352.0000 - val_mse: 74272423872640843776.0000\n",
      "Epoch 110/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 81876772799710658560.0000 - mse: 81876778231734992896.0000 - val_loss: 78123428688824647680.0000 - val_mse: 78123423766507683840.0000\n",
      "Epoch 111/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 81504740203108286464.0000 - mse: 81504729885174661120.0000 - val_loss: 74996263185375969280.0000 - val_mse: 74996263163531362304.0000\n",
      "Epoch 112/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 80031454559835226112.0000 - mse: 80031445876605976576.0000 - val_loss: 73224618078914822144.0000 - val_mse: 73224628873788915712.0000\n",
      "Epoch 113/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 78465102367688835072.0000 - mse: 78465099203862331392.0000 - val_loss: 73079259710905409536.0000 - val_mse: 73079255844410884096.0000\n",
      "Epoch 114/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 77473247400320860160.0000 - mse: 77473234162492112896.0000 - val_loss: 70643576669680697344.0000 - val_mse: 70643578104142888960.0000\n",
      "Epoch 115/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - 1s 89us/step - loss: 75043813051152056320.0000 - mse: 75043806046316396544.0000 - val_loss: 69893097573696028672.0000 - val_mse: 69893091049441591296.0000\n",
      "Epoch 116/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 74432216429057507328.0000 - mse: 74432213698482274304.0000 - val_loss: 71830582381230931968.0000 - val_mse: 71830575673117769728.0000\n",
      "Epoch 117/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 74380893614616068096.0000 - mse: 74380870903511646208.0000 - val_loss: 70588034868929732608.0000 - val_mse: 70588026378661134336.0000\n",
      "Epoch 118/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 71583644078702313472.0000 - mse: 71583647351751835648.0000 - val_loss: 68728612334183800832.0000 - val_mse: 68728611478603563008.0000\n",
      "Epoch 119/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 69279708267066941440.0000 - mse: 69279713094723960832.0000 - val_loss: 67760362592632659968.0000 - val_mse: 67760368345044484096.0000\n",
      "Epoch 120/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 69776205150323064832.0000 - mse: 69776199769269469184.0000 - val_loss: 64487154185987489792.0000 - val_mse: 64487157413527420928.0000\n",
      "Epoch 121/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 66708480399595094016.0000 - mse: 66708478764890718208.0000 - val_loss: 67821769530638286848.0000 - val_mse: 67821765074339495936.0000\n",
      "Epoch 122/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 65499711885100630016.0000 - mse: 65499715263732383744.0000 - val_loss: 64859275939512041472.0000 - val_mse: 64859271730785419264.0000\n",
      "Epoch 123/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 65909563105254408192.0000 - mse: 65909573616148676608.0000 - val_loss: 62904763559716651008.0000 - val_mse: 62904757871018246144.0000\n",
      "Epoch 124/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 64543849470916329472.0000 - mse: 64543848233055551488.0000 - val_loss: 61875493418569711616.0000 - val_mse: 61875496240164110336.0000\n",
      "Epoch 125/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 64311312202150477824.0000 - mse: 64311314717920460800.0000 - val_loss: 62509243525211136000.0000 - val_mse: 62509241548274663424.0000\n",
      "Epoch 126/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 62566861779156525056.0000 - mse: 62566869151709659136.0000 - val_loss: 62998740444145123328.0000 - val_mse: 62998739726914027520.0000\n",
      "Epoch 127/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 61063612871215243264.0000 - mse: 61063621252260823040.0000 - val_loss: 61592541706723770368.0000 - val_mse: 61592539121779212288.0000\n",
      "Epoch 128/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 60889867767095787520.0000 - mse: 60889867628746637312.0000 - val_loss: 65294958980266565632.0000 - val_mse: 65294964208407937024.0000\n",
      "Epoch 129/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 59805797163631353856.0000 - mse: 59805793144224612352.0000 - val_loss: 60522075586357379072.0000 - val_mse: 60522072193162543104.0000\n",
      "Epoch 130/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 58621035324124717056.0000 - mse: 58621034180923883520.0000 - val_loss: 59201049477742706688.0000 - val_mse: 59201039758715256832.0000\n",
      "Epoch 131/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 58439616835757637632.0000 - mse: 58439614762340843520.0000 - val_loss: 59410647888451575808.0000 - val_mse: 59410646257387962368.0000\n",
      "Epoch 132/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 57787006134443458560.0000 - mse: 57787006232744165376.0000 - val_loss: 58543522199141531648.0000 - val_mse: 58543518611165675520.0000\n",
      "Epoch 133/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 57127212602183688192.0000 - mse: 57127206897101832192.0000 - val_loss: 58269915759070126080.0000 - val_mse: 58269916137709895680.0000\n",
      "Epoch 134/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 56463739947464400896.0000 - mse: 56463743988715749376.0000 - val_loss: 59717923779292061696.0000 - val_mse: 59717920174932754432.0000\n",
      "Epoch 135/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 55619212922944790528.0000 - mse: 55619209107421003776.0000 - val_loss: 56570964441550430208.0000 - val_mse: 56570968362656464896.0000\n",
      "Epoch 136/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 54753678498197299200.0000 - mse: 54753686748175269888.0000 - val_loss: 56083446181169029120.0000 - val_mse: 56083449304947097600.0000\n",
      "Epoch 137/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 55279922757741682688.0000 - mse: 55279926207368396800.0000 - val_loss: 54508483251088121856.0000 - val_mse: 54508486859088199680.0000\n",
      "Epoch 138/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 53333545941178490880.0000 - mse: 53333535539507232768.0000 - val_loss: 54667518229430181888.0000 - val_mse: 54667515822883209216.0000\n",
      "Epoch 139/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 54360687837400350720.0000 - mse: 54360686108036038656.0000 - val_loss: 54254137751708057600.0000 - val_mse: 54254143431304544256.0000\n",
      "Epoch 140/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 50810603651110862848.0000 - mse: 50810596158412423168.0000 - val_loss: 56319480024338046976.0000 - val_mse: 56319483665105027072.0000\n",
      "Epoch 141/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 52197650891116675072.0000 - mse: 52197643270991380480.0000 - val_loss: 53833117288314191872.0000 - val_mse: 53833118438796558336.0000\n",
      "Epoch 142/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 51092941766293929984.0000 - mse: 51092941948332277760.0000 - val_loss: 54665680526811963392.0000 - val_mse: 54665681837488078848.0000\n",
      "Epoch 143/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 51274470098421161984.0000 - mse: 51274453725892050944.0000 - val_loss: 55422779971627368448.0000 - val_mse: 55422774758049054720.0000\n",
      "Epoch 144/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 50704491520560054272.0000 - mse: 50704488888285528064.0000 - val_loss: 58246601233933524992.0000 - val_mse: 58246597695108022272.0000\n",
      "Epoch 145/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 51287664518293528576.0000 - mse: 51287665457611407360.0000 - val_loss: 55689747919105810432.0000 - val_mse: 55689753773459111936.0000\n",
      "Epoch 146/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 49756280119930667008.0000 - mse: 49756274458538016768.0000 - val_loss: 52061411046093930496.0000 - val_mse: 52061409382263422976.0000\n",
      "Epoch 147/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 49094016239896977408.0000 - mse: 49094016614895976448.0000 - val_loss: 52753701055919292416.0000 - val_mse: 52753697087483281408.0000\n",
      "Epoch 148/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 50250499710722711552.0000 - mse: 50250496139130306560.0000 - val_loss: 55132265247205179392.0000 - val_mse: 55132266193804591104.0000\n",
      "Epoch 149/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 49265782765560717312.0000 - mse: 49265777923340632064.0000 - val_loss: 51910725602500222976.0000 - val_mse: 51910719114653466624.0000\n",
      "Epoch 150/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 48484692570472251392.0000 - mse: 48484693659061583872.0000 - val_loss: 51413180229324775424.0000 - val_mse: 51413176908945293312.0000\n",
      "Epoch 151/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 49091827574623477760.0000 - mse: 49091830785779957760.0000 - val_loss: 49531132057166249984.0000 - val_mse: 49531129661541580800.0000\n",
      "Epoch 152/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 47913557324582567936.0000 - mse: 47913558941083107328.0000 - val_loss: 50825360072881168384.0000 - val_mse: 50825360400550199296.0000\n",
      "Epoch 153/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 48056566826289856512.0000 - mse: 48056552627298631680.0000 - val_loss: 48971903462742188032.0000 - val_mse: 48971904853515173888.0000\n",
      "Epoch 154/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 47741080609067229184.0000 - mse: 47741076353010630656.0000 - val_loss: 48664337526559514624.0000 - val_mse: 48664336266854137856.0000\n",
      "Epoch 155/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 48070561537715027968.0000 - mse: 48070560405436497920.0000 - val_loss: 52640497100854689792.0000 - val_mse: 52640500166380486656.0000\n",
      "Epoch 156/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 46189160253392633856.0000 - mse: 46189159670869917696.0000 - val_loss: 49428153167037136896.0000 - val_mse: 49428158198577102848.0000\n",
      "Epoch 157/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 47309019068353257472.0000 - mse: 47309012263759773696.0000 - val_loss: 48709881661039763456.0000 - val_mse: 48709882436523130880.0000\n",
      "Epoch 158/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 46993170037778841600.0000 - mse: 46993170951611351040.0000 - val_loss: 49906811988691656704.0000 - val_mse: 49906810794520084480.0000\n",
      "Epoch 159/500\n",
      "6040/6040 [==============================] - 1s 92us/step - loss: 45641361668502781952.0000 - mse: 45641360987679358976.0000 - val_loss: 49402621917235929088.0000 - val_mse: 49402618742487121920.0000\n",
      "Epoch 160/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 45389864119043907584.0000 - mse: 45389863095988387840.0000 - val_loss: 49441218772933009408.0000 - val_mse: 49441215998668570624.0000\n",
      "Epoch 161/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 44241140258570551296.0000 - mse: 44241150521892667392.0000 - val_loss: 47233088473732628480.0000 - val_mse: 47233088786838585344.0000\n",
      "Epoch 162/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 45220501169487691776.0000 - mse: 45220507518985306112.0000 - val_loss: 46755483705089376256.0000 - val_mse: 46755482925965246464.0000\n",
      "Epoch 163/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 45180550170561060864.0000 - mse: 45180546868385415168.0000 - val_loss: 49680108164303904768.0000 - val_mse: 49680109089058717696.0000\n",
      "Epoch 164/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 44106326622118952960.0000 - mse: 44106319610001752064.0000 - val_loss: 45807389974048768000.0000 - val_mse: 45807387243473534976.0000\n",
      "Epoch 165/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 43951566042559848448.0000 - mse: 43951561149369024512.0000 - val_loss: 48939640908095791104.0000 - val_mse: 48939640784309714944.0000\n",
      "Epoch 166/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 43038830319066849280.0000 - mse: 43038838954966122496.0000 - val_loss: 48992312080588054528.0000 - val_mse: 48992311789326696448.0000\n",
      "Epoch 167/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 43928896475652874240.0000 - mse: 43928906811790327808.0000 - val_loss: 45617542012433162240.0000 - val_mse: 45617541167775219712.0000\n",
      "Epoch 168/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 43686411248007880704.0000 - mse: 43686416119354097664.0000 - val_loss: 47637733280601333760.0000 - val_mse: 47637735454139219968.0000\n",
      "Epoch 169/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 43821280995960168448.0000 - mse: 43821277817570590720.0000 - val_loss: 45901563312443064320.0000 - val_mse: 45901567011462316032.0000\n",
      "Epoch 170/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 42380219934133911552.0000 - mse: 42380218295788765184.0000 - val_loss: 44960491361200324608.0000 - val_mse: 44960495009248837632.0000\n",
      "Epoch 171/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 42109199394319204352.0000 - mse: 42109206271728025600.0000 - val_loss: 46562782518081224704.0000 - val_mse: 46562786916127735808.0000\n",
      "Epoch 172/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 43105824611088130048.0000 - mse: 43105829999423258624.0000 - val_loss: 45562338999612686336.0000 - val_mse: 45562336887967842304.0000\n",
      "Epoch 173/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 42980807878093709312.0000 - mse: 42980802333205594112.0000 - val_loss: 44192454433448443904.0000 - val_mse: 44192455350921723904.0000\n",
      "Epoch 174/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 41790921761005043712.0000 - mse: 41790915247672918016.0000 - val_loss: 43650682093392855040.0000 - val_mse: 43650681991451377664.0000\n",
      "Epoch 175/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 41937375894293004288.0000 - mse: 41937365798446170112.0000 - val_loss: 44767767677893001216.0000 - val_mse: 44767768213085749248.0000\n",
      "Epoch 176/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 42237930645309718528.0000 - mse: 42237928297015017472.0000 - val_loss: 44540921466749460480.0000 - val_mse: 44540921372089516032.0000\n",
      "Epoch 177/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 42438652747797897216.0000 - mse: 42438650741735292928.0000 - val_loss: 42514429388946038784.0000 - val_mse: 42514429083121614848.0000\n",
      "Epoch 178/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 42725402875474198528.0000 - mse: 42725398976212762624.0000 - val_loss: 42638333995055104000.0000 - val_mse: 42638330849432436736.0000\n",
      "Epoch 179/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 41143875882820067328.0000 - mse: 41143887839098830848.0000 - val_loss: 42871663097872064512.0000 - val_mse: 42871664809032548352.0000\n",
      "Epoch 180/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 42068705396842201088.0000 - mse: 42068704661407268864.0000 - val_loss: 41922845880898846720.0000 - val_mse: 41922847846913015808.0000\n",
      "Epoch 181/500\n",
      "6040/6040 [==============================] - 1s 91us/step - loss: 41014170963312418816.0000 - mse: 41014176253346840576.0000 - val_loss: 44184501177981968384.0000 - val_mse: 44184503682829647872.0000\n",
      "Epoch 182/500\n",
      "6040/6040 [==============================] - 1s 92us/step - loss: 41410404670063157248.0000 - mse: 41410400661578711040.0000 - val_loss: 42352360428412444672.0000 - val_mse: 42352361069187432448.0000\n",
      "Epoch 183/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 40707020300292947968.0000 - mse: 40707025481104359424.0000 - val_loss: 42840281339417927680.0000 - val_mse: 42840280349129310208.0000\n",
      "Epoch 184/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 40059134089399926784.0000 - mse: 40059144851507118080.0000 - val_loss: 41664624810554589184.0000 - val_mse: 41664620944060055552.0000\n",
      "Epoch 185/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 39802571788899147776.0000 - mse: 39802567216095821824.0000 - val_loss: 42042949569970634752.0000 - val_mse: 42042949701038243840.0000\n",
      "Epoch 186/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 39273491353673973760.0000 - mse: 39273499812996055040.0000 - val_loss: 42366726021129043968.0000 - val_mse: 42366725089092698112.0000\n",
      "Epoch 187/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 39682326903602364416.0000 - mse: 39682324624482238464.0000 - val_loss: 40232192260887240704.0000 - val_mse: 40232194787579527168.0000\n",
      "Epoch 188/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 38935010367135866880.0000 - mse: 38935012959361957888.0000 - val_loss: 40534179231381381120.0000 - val_mse: 40534182253217972224.0000\n",
      "Epoch 189/500\n",
      "6040/6040 [==============================] - 1s 85us/step - loss: 39887544348458409984.0000 - mse: 39887541872736862208.0000 - val_loss: 40584067401594593280.0000 - val_mse: 40584073692839936000.0000\n",
      "Epoch 190/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 39502615802060627968.0000 - mse: 39502624842085040128.0000 - val_loss: 40525601504281059328.0000 - val_mse: 40525601664474808320.0000\n",
      "Epoch 191/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - 1s 86us/step - loss: 38127589009333157888.0000 - mse: 38127588794527907840.0000 - val_loss: 39609309188341235712.0000 - val_mse: 39609308256304889856.0000\n",
      "Epoch 192/500\n",
      "6040/6040 [==============================] - 1s 85us/step - loss: 38807101993196118016.0000 - mse: 38807104572679520256.0000 - val_loss: 39066072903164764160.0000 - val_mse: 39066074745392857088.0000\n",
      "Epoch 193/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 39560878388023738368.0000 - mse: 39560881366171123712.0000 - val_loss: 39666932168002166784.0000 - val_mse: 39666931461693374464.0000\n",
      "Epoch 194/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 38752485888396173312.0000 - mse: 38752485233058119680.0000 - val_loss: 38986879784486993920.0000 - val_mse: 38986879121867407360.0000\n",
      "Epoch 195/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 39810702619134279680.0000 - mse: 39810708000187875328.0000 - val_loss: 39562111086189150208.0000 - val_mse: 39562112819194232832.0000\n",
      "Epoch 196/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 37416160845503225856.0000 - mse: 37416160790891724800.0000 - val_loss: 39058322598612787200.0000 - val_mse: 39058320989393780736.0000\n",
      "Epoch 197/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 39325625219955040256.0000 - mse: 39325629858292170752.0000 - val_loss: 38972420209011589120.0000 - val_mse: 38972418344938897408.0000\n",
      "Epoch 198/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 38135671107902275584.0000 - mse: 38135672404015316992.0000 - val_loss: 39777036512409657344.0000 - val_mse: 39777036556098863104.0000\n",
      "Epoch 199/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 36493637182121365504.0000 - mse: 36493639748862083072.0000 - val_loss: 37539644203950481408.0000 - val_mse: 37539649140830502912.0000\n",
      "Epoch 200/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 37422989063924637696.0000 - mse: 37422982161030447104.0000 - val_loss: 38550435732414619648.0000 - val_mse: 38550438976338001920.0000\n",
      "Epoch 201/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 37361565301934809088.0000 - mse: 37361572237595901952.0000 - val_loss: 38538073664689078272.0000 - val_mse: 38538071669548777472.0000\n",
      "Epoch 202/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 38419934861148405760.0000 - mse: 38419935742214012928.0000 - val_loss: 39897372467851665408.0000 - val_mse: 39897375904735690752.0000\n",
      "Epoch 203/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 37990507502722433024.0000 - mse: 37990510480869818368.0000 - val_loss: 39474408919839367168.0000 - val_mse: 39474406975669796864.0000\n",
      "Epoch 204/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 37419795896478457856.0000 - mse: 37419793577309896704.0000 - val_loss: 36589054414039535616.0000 - val_mse: 36589050969873973248.0000\n",
      "Epoch 205/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 35938911666694991872.0000 - mse: 35938909744370024448.0000 - val_loss: 39489748395878350848.0000 - val_mse: 39489747361900527616.0000\n",
      "Epoch 206/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 35218295106278518784.0000 - mse: 35218292022548889600.0000 - val_loss: 36956596561464877056.0000 - val_mse: 36956595716806934528.0000\n",
      "Epoch 207/500\n",
      "6040/6040 [==============================] - 1s 90us/step - loss: 36309168289883283456.0000 - mse: 36309165886977081344.0000 - val_loss: 36194831757745205248.0000 - val_mse: 36194829871827910656.0000\n",
      "Epoch 208/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 36202540630368960512.0000 - mse: 36202544045408387072.0000 - val_loss: 36399463119214436352.0000 - val_mse: 36399462179896557568.0000\n",
      "Epoch 209/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 36840786563600265216.0000 - mse: 36840790754123055104.0000 - val_loss: 37349172294971473920.0000 - val_mse: 37349169746434588672.0000\n",
      "Epoch 210/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 36521816598396538880.0000 - mse: 36521813634812215296.0000 - val_loss: 35653963689547354112.0000 - val_mse: 35653966907985362944.0000\n",
      "Epoch 211/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 35214731392491483136.0000 - mse: 35214731803898150912.0000 - val_loss: 35158528424816287744.0000 - val_mse: 35158529167532752896.0000\n",
      "Epoch 212/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 36105901491518697472.0000 - mse: 36105903570396643328.0000 - val_loss: 34933820068402274304.0000 - val_mse: 34933821976164171776.0000\n",
      "Epoch 213/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 35822253584929431552.0000 - mse: 35822249361639735296.0000 - val_loss: 34390217194398654464.0000 - val_mse: 34390214631298695168.0000\n",
      "Epoch 214/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 34242014906797096960.0000 - mse: 34242022454107045888.0000 - val_loss: 35076758618842669056.0000 - val_mse: 35076756288751796224.0000\n",
      "Epoch 215/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 34092235431658541056.0000 - mse: 34092231587008610304.0000 - val_loss: 36152272094666338304.0000 - val_mse: 36152269975739957248.0000\n",
      "Epoch 216/500\n",
      "6040/6040 [==============================] - 1s 84us/step - loss: 35115391751141928960.0000 - mse: 35115395326375100416.0000 - val_loss: 36169484497744068608.0000 - val_mse: 36169481730761162752.0000\n",
      "Epoch 217/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 33692385023398150144.0000 - mse: 33692376591381823488.0000 - val_loss: 37642631912017215488.0000 - val_mse: 37642629399888003072.0000\n",
      "Epoch 218/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 33679515487367184384.0000 - mse: 33679516703383355392.0000 - val_loss: 35437742536203481088.0000 - val_mse: 35437739150290190336.0000\n",
      "Epoch 219/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 35223810045438959616.0000 - mse: 35223809371897069568.0000 - val_loss: 33943032461048864768.0000 - val_mse: 33943030058142662656.0000\n",
      "Epoch 220/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 33997377807469703168.0000 - mse: 33997372320833863680.0000 - val_loss: 33492977580021624832.0000 - val_mse: 33492977958661390336.0000\n",
      "Epoch 221/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 32843449849239072768.0000 - mse: 32843454658692251648.0000 - val_loss: 33213788873906413568.0000 - val_mse: 33213787767113252864.0000\n",
      "Epoch 222/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 34709055596846809088.0000 - mse: 34709059809214201856.0000 - val_loss: 33416211534961463296.0000 - val_mse: 33416210056810070016.0000\n",
      "Epoch 223/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 33849766004942237696.0000 - mse: 33849767282851446784.0000 - val_loss: 33670059294165475328.0000 - val_mse: 33670058704361226240.0000\n",
      "Epoch 224/500\n",
      "6040/6040 [==============================] - 1s 91us/step - loss: 33084943983656165376.0000 - mse: 33084951392616972288.0000 - val_loss: 33685824710814916608.0000 - val_mse: 33685825701103534080.0000\n",
      "Epoch 225/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 33622110312950030336.0000 - mse: 33622100206180892672.0000 - val_loss: 33905938564931616768.0000 - val_mse: 33905934734844755968.0000\n",
      "Epoch 226/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 33592986671040282624.0000 - mse: 33592991735347150848.0000 - val_loss: 33054998263844012032.0000 - val_mse: 33055000695876354048.0000\n",
      "Epoch 227/500\n",
      "6040/6040 [==============================] - 1s 85us/step - loss: 33250106130713538560.0000 - mse: 33250106835201949696.0000 - val_loss: 33393539590633660416.0000 - val_mse: 33393538127045328896.0000\n",
      "Epoch 228/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 33466731254459523072.0000 - mse: 33466719421966843904.0000 - val_loss: 34438491623774380032.0000 - val_mse: 34438491987851083776.0000\n",
      "Epoch 229/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 33068870969526943744.0000 - mse: 33068876532618887168.0000 - val_loss: 33998165716774014976.0000 - val_mse: 33998161770182606848.0000\n",
      "Epoch 230/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 33043652093891796992.0000 - mse: 33043655934900961280.0000 - val_loss: 32376193971657039872.0000 - val_mse: 32376190804189773824.0000\n",
      "Epoch 231/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 32836816939762270208.0000 - mse: 32836815807483740160.0000 - val_loss: 32608757210013753344.0000 - val_mse: 32608757304673697792.0000\n",
      "Epoch 232/500\n",
      "6040/6040 [==============================] - 1s 90us/step - loss: 32552333109433769984.0000 - mse: 32552330367936233472.0000 - val_loss: 32945394743342211072.0000 - val_mse: 32945396978773131264.0000\n",
      "Epoch 233/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 32595440423963209728.0000 - mse: 32595440019838074880.0000 - val_loss: 33459323509672632320.0000 - val_mse: 33459324106758422528.0000\n",
      "Epoch 234/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 33709371702563921920.0000 - mse: 33709366245054218240.0000 - val_loss: 32501897779273519104.0000 - val_mse: 32501897968593403904.0000\n",
      "Epoch 235/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 31561079076141101056.0000 - mse: 31561081053077569536.0000 - val_loss: 33654595671613255680.0000 - val_mse: 33654597371851440128.0000\n",
      "Epoch 236/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 32091116346797277184.0000 - mse: 32091116026409779200.0000 - val_loss: 32784947868284006400.0000 - val_mse: 32784947445955035136.0000\n",
      "Epoch 237/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 32797858890854699008.0000 - mse: 32797857911488380928.0000 - val_loss: 32167139078307852288.0000 - val_mse: 32167142857423978496.0000\n",
      "Epoch 238/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 31512219280609648640.0000 - mse: 31512225353408970752.0000 - val_loss: 33010833342914732032.0000 - val_mse: 33010835512811847680.0000\n",
      "Epoch 239/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 31560573344481529856.0000 - mse: 31560573078705537024.0000 - val_loss: 33607027875059785728.0000 - val_mse: 33607023702740828160.0000\n",
      "Epoch 240/500\n",
      "6040/6040 [==============================] - 1s 91us/step - loss: 32752920474417577984.0000 - mse: 32752916473214664704.0000 - val_loss: 33612549589687574528.0000 - val_mse: 33612549848182030336.0000\n",
      "Epoch 241/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 31739780785960099840.0000 - mse: 31739778080870236160.0000 - val_loss: 32660070201665892352.0000 - val_mse: 32660069313318748160.0000\n",
      "Epoch 242/500\n",
      "6040/6040 [==============================] - 1s 90us/step - loss: 32405722146467405824.0000 - mse: 32405719288465326080.0000 - val_loss: 31563013870733123584.0000 - val_mse: 31563011795495944192.0000\n",
      "Epoch 243/500\n",
      "6040/6040 [==============================] - 1s 92us/step - loss: 31273037025942929408.0000 - mse: 31273037593902579712.0000 - val_loss: 33280572773285486592.0000 - val_mse: 33280574302407622656.0000\n",
      "Epoch 244/500\n",
      "6040/6040 [==============================] - 1s 93us/step - loss: 32291613413478453248.0000 - mse: 32291618568804499456.0000 - val_loss: 31917198954302820352.0000 - val_mse: 31917197476151427072.0000\n",
      "Epoch 245/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 31195290012627841024.0000 - mse: 31195293325725794304.0000 - val_loss: 31598700777888120832.0000 - val_mse: 31598697544887042048.0000\n",
      "Epoch 246/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 31369320938799783936.0000 - mse: 31369321827146924032.0000 - val_loss: 32672146021369208832.0000 - val_mse: 32672148548061495296.0000\n",
      "Epoch 247/500\n",
      "6040/6040 [==============================] - 1s 92us/step - loss: 32162075838184247296.0000 - mse: 32162078506866442240.0000 - val_loss: 30937402764207923200.0000 - val_mse: 30937405072454189056.0000\n",
      "Epoch 248/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 31287250576589705216.0000 - mse: 31287249881203212288.0000 - val_loss: 32638441475426824192.0000 - val_mse: 32638441919600394240.0000\n",
      "Epoch 249/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 31621949587180744704.0000 - mse: 31621947817767993344.0000 - val_loss: 35967922191795068928.0000 - val_mse: 35967921458180521984.0000\n",
      "Epoch 250/500\n",
      "6040/6040 [==============================] - 1s 85us/step - loss: 31752866244958314496.0000 - mse: 31752862269240770560.0000 - val_loss: 32471324802640785408.0000 - val_mse: 32471327147294720000.0000\n",
      "Epoch 251/500\n",
      "6040/6040 [==============================] - 1s 85us/step - loss: 31238377980121559040.0000 - mse: 31238380987395080192.0000 - val_loss: 33140424113455276032.0000 - val_mse: 33140426152284782592.0000\n",
      "Epoch 252/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 32598929378241093632.0000 - mse: 32598936466814402560.0000 - val_loss: 32005204571298803712.0000 - val_mse: 32005204585861873664.0000\n",
      "Epoch 253/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 31857444749749022720.0000 - mse: 31857450014298079232.0000 - val_loss: 31476669215322333184.0000 - val_mse: 31476671545413206016.0000\n",
      "Epoch 254/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 31123756379569754112.0000 - mse: 31123759099222687744.0000 - val_loss: 31239413800171790336.0000 - val_mse: 31239416727348445184.0000\n",
      "Epoch 255/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 32257847469641728000.0000 - mse: 32257852566715498496.0000 - val_loss: 30801011907452416000.0000 - val_mse: 30801010655028576256.0000\n",
      "Epoch 256/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 31311723408100036608.0000 - mse: 31311720611990994944.0000 - val_loss: 31456599676668334080.0000 - val_mse: 31456598861136527360.0000\n",
      "Epoch 257/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 30241014215567024128.0000 - mse: 30241018387885981696.0000 - val_loss: 31441074397727318016.0000 - val_mse: 31441071557929074688.0000\n",
      "Epoch 258/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 31163440397413015552.0000 - mse: 31163438274845868032.0000 - val_loss: 32090101938097639424.0000 - val_mse: 32090104475712225280.0000\n",
      "Epoch 259/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 31466274461519478784.0000 - mse: 31466267966391189504.0000 - val_loss: 30409698158440816640.0000 - val_mse: 30409696665726353408.0000\n",
      "Epoch 260/500\n",
      "6040/6040 [==============================] - 1s 85us/step - loss: 31092483898736500736.0000 - mse: 31092488988528738304.0000 - val_loss: 34256838336883662848.0000 - val_mse: 34256846069872721920.0000\n",
      "Epoch 261/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 32777768841939910656.0000 - mse: 32777769834048913408.0000 - val_loss: 30154692409609801728.0000 - val_mse: 30154695729989287936.0000\n",
      "Epoch 262/500\n",
      "6040/6040 [==============================] - 1s 90us/step - loss: 31031277690777522176.0000 - mse: 31031274778163937280.0000 - val_loss: 31233979933610450944.0000 - val_mse: 31233978542837465088.0000\n",
      "Epoch 263/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 30743340683988242432.0000 - mse: 30743343469174980608.0000 - val_loss: 30840225807618695168.0000 - val_mse: 30840230234791346176.0000\n",
      "Epoch 264/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 30923634279120543744.0000 - mse: 30923625992734900224.0000 - val_loss: 30418823857379201024.0000 - val_mse: 30418824811260149760.0000\n",
      "Epoch 265/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 30675667665623597056.0000 - mse: 30675675125555134464.0000 - val_loss: 29343116794693697536.0000 - val_mse: 29343117610225500160.0000\n",
      "Epoch 266/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 30337848827196411904.0000 - mse: 30337854575967469568.0000 - val_loss: 30498887141209645056.0000 - val_mse: 30498886849948286976.0000\n",
      "Epoch 267/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - 1s 87us/step - loss: 30061468204603965440.0000 - mse: 30061476935163183104.0000 - val_loss: 30275941236401152000.0000 - val_mse: 30275943275230658560.0000\n",
      "Epoch 268/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 30705245442243280896.0000 - mse: 30705247590295797760.0000 - val_loss: 30131728519459450880.0000 - val_mse: 30131726932085047296.0000\n",
      "Epoch 269/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 31128270460965257216.0000 - mse: 31128271494943080448.0000 - val_loss: 29850102480045285376.0000 - val_mse: 29850102421793013760.0000\n",
      "Epoch 270/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 30614876097334362112.0000 - mse: 30614878729608888320.0000 - val_loss: 29891023186334572544.0000 - val_mse: 29891024045555580928.0000\n",
      "Epoch 271/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 30424541625488445440.0000 - mse: 30424544470747840512.0000 - val_loss: 30617875262871281664.0000 - val_mse: 30617873799282950144.0000\n",
      "Epoch 272/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 30407407274993602560.0000 - mse: 30407409681540579328.0000 - val_loss: 30948917876670279680.0000 - val_mse: 30948919158220259328.0000\n",
      "Epoch 273/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 30286400566487920640.0000 - mse: 30286399630810808320.0000 - val_loss: 30982796600545730560.0000 - val_mse: 30982799509518548992.0000\n",
      "Epoch 274/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 29665324333778444288.0000 - mse: 29665325094698745856.0000 - val_loss: 30296032146357329920.0000 - val_mse: 30296033551693381632.0000\n",
      "Epoch 275/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 29362850549049962496.0000 - mse: 29362851644920823808.0000 - val_loss: 29872557667670208512.0000 - val_mse: 29872558847278710784.0000\n",
      "Epoch 276/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 30558274964460003328.0000 - mse: 30558271472964468736.0000 - val_loss: 31292190602236428288.0000 - val_mse: 31292191086458437632.0000\n",
      "Epoch 277/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 30359006321683800064.0000 - mse: 30359006980662624256.0000 - val_loss: 29546450208171708416.0000 - val_mse: 29546450295550115840.0000\n",
      "Epoch 278/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 29405725899392843776.0000 - mse: 29405728200357576704.0000 - val_loss: 30154272221411172352.0000 - val_mse: 30154271318500966400.0000\n",
      "Epoch 279/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 29828039086277316608.0000 - mse: 29828035223423549440.0000 - val_loss: 29657407074575097856.0000 - val_mse: 29657408610978758656.0000\n",
      "Epoch 280/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 29281098990600437760.0000 - mse: 29281098557349167104.0000 - val_loss: 29217177626055073792.0000 - val_mse: 29217177349356781568.0000\n",
      "Epoch 281/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 30497473496925356032.0000 - mse: 30497475077018222592.0000 - val_loss: 32844885634847748096.0000 - val_mse: 32844886222831616000.0000\n",
      "Epoch 282/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 30016511988457082880.0000 - mse: 30016517904703422464.0000 - val_loss: 30067916141273612288.0000 - val_mse: 30067920073301950464.0000\n",
      "Epoch 283/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 30174275243252473856.0000 - mse: 30174267036963700736.0000 - val_loss: 30203264537219911680.0000 - val_mse: 30203263357611409408.0000\n",
      "Epoch 284/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 30341384700038356992.0000 - mse: 30341388406339141632.0000 - val_loss: 30372251428314062848.0000 - val_mse: 30372251697730813952.0000\n",
      "Epoch 285/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 29242265412234354688.0000 - mse: 29242268204702629888.0000 - val_loss: 31092244634812145664.0000 - val_mse: 31092242697924116480.0000\n",
      "Epoch 286/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 30222286710951591936.0000 - mse: 30222282709748678656.0000 - val_loss: 29628447256257114112.0000 - val_mse: 29628447474703138816.0000\n",
      "Epoch 287/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 30492627264717553664.0000 - mse: 30492628429762985984.0000 - val_loss: 29005793850573705216.0000 - val_mse: 29005794039893590016.0000\n",
      "Epoch 288/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 29860704848585142272.0000 - mse: 29860706111931285504.0000 - val_loss: 30186467185218605056.0000 - val_mse: 30186465018962247680.0000\n",
      "Epoch 289/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 28831759622226120704.0000 - mse: 28831754543356182528.0000 - val_loss: 30545585562656878592.0000 - val_mse: 30545583108779933696.0000\n",
      "Epoch 290/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 28139064169308401664.0000 - mse: 28139066615903813632.0000 - val_loss: 28802408607136976896.0000 - val_mse: 28802408578010841088.0000\n",
      "Epoch 291/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 29368895623931039744.0000 - mse: 29368892361803825152.0000 - val_loss: 28809518347865067520.0000 - val_mse: 28809515821172785152.0000\n",
      "Epoch 292/500\n",
      "6040/6040 [==============================] - 1s 90us/step - loss: 29342218686654451712.0000 - mse: 29342216010690723840.0000 - val_loss: 29481888785869066240.0000 - val_mse: 29481886972767109120.0000\n",
      "Epoch 293/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 29013153817575526400.0000 - mse: 29013158568776433664.0000 - val_loss: 28638554076127543296.0000 - val_mse: 28638559355239661568.0000\n",
      "Epoch 294/500\n",
      "6040/6040 [==============================] - 1s 93us/step - loss: 28920073153448615936.0000 - mse: 28920071715345661952.0000 - val_loss: 29282176348161142784.0000 - val_mse: 29282178277767643136.0000\n",
      "Epoch 295/500\n",
      "6040/6040 [==============================] - 1s 91us/step - loss: 29113912638995566592.0000 - mse: 29113913416299315200.0000 - val_loss: 29412384960498860032.0000 - val_mse: 29412386842775388160.0000\n",
      "Epoch 296/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 29230607760103989248.0000 - mse: 29230604585355182080.0000 - val_loss: 28201312240990810112.0000 - val_mse: 28201314367198724096.0000\n",
      "Epoch 297/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 28109259653002776576.0000 - mse: 28109258855674806272.0000 - val_loss: 29009575668147412992.0000 - val_mse: 29009574160869883904.0000\n",
      "Epoch 298/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 28184493437590454272.0000 - mse: 28184496237340262400.0000 - val_loss: 30340219144897572864.0000 - val_mse: 30340220724990443520.0000\n",
      "Epoch 299/500\n",
      "6040/6040 [==============================] - 1s 90us/step - loss: 27968593075865907200.0000 - mse: 27968598333133422592.0000 - val_loss: 29602004387530407936.0000 - val_mse: 29602004220055126016.0000\n",
      "Epoch 300/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 28460792044625154048.0000 - mse: 28460792514284093440.0000 - val_loss: 31364123277918527488.0000 - val_mse: 31364123336170799104.0000\n",
      "Epoch 301/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 28927486657928867840.0000 - mse: 28927491219809894400.0000 - val_loss: 29683870342519193600.0000 - val_mse: 29683869457812815872.0000\n",
      "Epoch 302/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 28235390739211567104.0000 - mse: 28235390431566757888.0000 - val_loss: 28066118362835881984.0000 - val_mse: 28066120616470642688.0000\n",
      "Epoch 303/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 28553347908575432704.0000 - mse: 28553347204087021568.0000 - val_loss: 28721987392223256576.0000 - val_mse: 28721990297555304448.0000\n",
      "Epoch 304/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 27961924840027103232.0000 - mse: 27961928695599333376.0000 - val_loss: 28851356341754048512.0000 - val_mse: 28851356636656173056.0000\n",
      "Epoch 305/500\n",
      "6040/6040 [==============================] - 1s 85us/step - loss: 29790005058567962624.0000 - mse: 29790003116218777600.0000 - val_loss: 29800088954584203264.0000 - val_mse: 29800090035891994624.0000\n",
      "Epoch 306/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 27820624285213810688.0000 - mse: 27820626058267328512.0000 - val_loss: 28729427641777737728.0000 - val_mse: 28729427394205581312.0000\n",
      "Epoch 307/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 28173365473366384640.0000 - mse: 28173366980643913728.0000 - val_loss: 27862343670196133888.0000 - val_mse: 27862341529425149952.0000\n",
      "Epoch 308/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 28311374654684999680.0000 - mse: 28311377680162357248.0000 - val_loss: 29346144594862903296.0000 - val_mse: 29346143466225139712.0000\n",
      "Epoch 309/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 28586561469784899584.0000 - mse: 28586556853292367872.0000 - val_loss: 28517258458198974464.0000 - val_mse: 28517259033440157696.0000\n",
      "Epoch 310/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 28256825876768423936.0000 - mse: 28256824311238623232.0000 - val_loss: 28572453031721586688.0000 - val_mse: 28572454517154512896.0000\n",
      "Epoch 311/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 28443718864476717056.0000 - mse: 28443723695774498816.0000 - val_loss: 28151545741569277952.0000 - val_mse: 28151543873855815680.0000\n",
      "Epoch 312/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 28962713510486814720.0000 - mse: 28962715174317326336.0000 - val_loss: 28720370374695493632.0000 - val_mse: 28720369617415962624.0000\n",
      "Epoch 313/500\n",
      "6040/6040 [==============================] - 1s 90us/step - loss: 28503318547598372864.0000 - mse: 28503323823069724672.0000 - val_loss: 27775230863923437568.0000 - val_mse: 27775231621202968576.0000\n",
      "Epoch 314/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 28072074948875522048.0000 - mse: 28072073372423421952.0000 - val_loss: 27522204803982069760.0000 - val_mse: 27522207607372644352.0000\n",
      "Epoch 315/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 28010872308868182016.0000 - mse: 28010874555221409792.0000 - val_loss: 29479899707441381376.0000 - val_mse: 29479899055744090112.0000\n",
      "Epoch 316/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 27149580258717278208.0000 - mse: 27149576519649591296.0000 - val_loss: 28223692643621208064.0000 - val_mse: 28223691627847221248.0000\n",
      "Epoch 317/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 27880603889651298304.0000 - mse: 27880602218539253760.0000 - val_loss: 27560744565171380224.0000 - val_mse: 27560743290902937600.0000\n",
      "Epoch 318/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 28010795279340818432.0000 - mse: 28010797589407465472.0000 - val_loss: 28068437884556152832.0000 - val_mse: 28068440586005250048.0000\n",
      "Epoch 319/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 27606534203850588160.0000 - mse: 27606535751176552448.0000 - val_loss: 27957960387018641408.0000 - val_mse: 27957959458623062016.0000\n",
      "Epoch 320/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 27917707837926178816.0000 - mse: 27917699740860416000.0000 - val_loss: 29049950790941773824.0000 - val_mse: 29049948227841818624.0000\n",
      "Epoch 321/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 27260700668373377024.0000 - mse: 27260704159868911616.0000 - val_loss: 28381026408058343424.0000 - val_mse: 28381022945688944640.0000\n",
      "Epoch 322/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 28052411395788152832.0000 - mse: 28052414104518787072.0000 - val_loss: 27636758820976431104.0000 - val_mse: 27636759126800859136.0000\n",
      "Epoch 323/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 27757233937454690304.0000 - mse: 27757239212926042112.0000 - val_loss: 28906874805188677632.0000 - val_mse: 28906875376789094400.0000\n",
      "Epoch 324/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 26694474155738222592.0000 - mse: 26694468865703804928.0000 - val_loss: 29371715063006015488.0000 - val_mse: 29371715907663953920.0000\n",
      "Epoch 325/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 27606495801040490496.0000 - mse: 27606493969734696960.0000 - val_loss: 27690676927032999936.0000 - val_mse: 27690681376050249728.0000\n",
      "Epoch 326/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 27688115793093681152.0000 - mse: 27688115115911020544.0000 - val_loss: 28278849021857439744.0000 - val_mse: 28278849728166232064.0000\n",
      "Epoch 327/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 28134125479823163392.0000 - mse: 28134127609671843840.0000 - val_loss: 27151998787454787584.0000 - val_mse: 27151999843277209600.0000\n",
      "Epoch 328/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 27683109676603977728.0000 - mse: 27683110138981384192.0000 - val_loss: 27425082000160894976.0000 - val_mse: 27425078949198168064.0000\n",
      "Epoch 329/500\n",
      "6040/6040 [==============================] - 1s 90us/step - loss: 28056710992319369216.0000 - mse: 28056715394006646784.0000 - val_loss: 28285833514740289536.0000 - val_mse: 28285833826025865216.0000\n",
      "Epoch 330/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 27599101286800801792.0000 - mse: 27599105251596042240.0000 - val_loss: 27900252379065290752.0000 - val_mse: 27900252690350866432.0000\n",
      "Epoch 331/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 27786527057067327488.0000 - mse: 27786523605620228096.0000 - val_loss: 27281488985448292352.0000 - val_mse: 27281489327680389120.0000\n",
      "Epoch 332/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 27909366884465594368.0000 - mse: 27909365442721873920.0000 - val_loss: 27230651376022192128.0000 - val_mse: 27230652308058537984.0000\n",
      "Epoch 333/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 28327541778999873536.0000 - mse: 28327542700113920000.0000 - val_loss: 26769315028753489920.0000 - val_mse: 26769314821229772800.0000\n",
      "Epoch 334/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 26934861897477959680.0000 - mse: 26934866088000749568.0000 - val_loss: 27009224741429592064.0000 - val_mse: 27009223860363984896.0000\n",
      "Epoch 335/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 27030103440544772096.0000 - mse: 27030103586175451136.0000 - val_loss: 26820176834716938240.0000 - val_mse: 26820176030107435008.0000\n",
      "Epoch 336/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 27062705999137841152.0000 - mse: 27062701906915753984.0000 - val_loss: 26788769427059429376.0000 - val_mse: 26788767380948385792.0000\n",
      "Epoch 337/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 27685230956741836800.0000 - mse: 27685232196422991872.0000 - val_loss: 28185936710436425728.0000 - val_mse: 28185936597572648960.0000\n",
      "Epoch 338/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 27610809108700147712.0000 - mse: 27610808453362089984.0000 - val_loss: 26557490505662365696.0000 - val_mse: 26557491707115470848.0000\n",
      "Epoch 339/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 26718864090625200128.0000 - mse: 26718867028724154368.0000 - val_loss: 26769132156668882944.0000 - val_mse: 26769130103276306432.0000\n",
      "Epoch 340/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 27409910696609837056.0000 - mse: 27409910086781370368.0000 - val_loss: 26724905721340715008.0000 - val_mse: 26724903347560644608.0000\n",
      "Epoch 341/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 27999369855352610816.0000 - mse: 27999364867501850624.0000 - val_loss: 26748276489045614592.0000 - val_mse: 26748276765743906816.0000\n",
      "Epoch 342/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 26656892356797300736.0000 - mse: 26656896354359443456.0000 - val_loss: 28702718057743646720.0000 - val_mse: 28702722455790157824.0000\n",
      "Epoch 343/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - 1s 88us/step - loss: 28369523422990794752.0000 - mse: 28369522054062407680.0000 - val_loss: 27196968077595893760.0000 - val_mse: 27196969868853248000.0000\n",
      "Epoch 344/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 27314298346887323648.0000 - mse: 27314300953676480512.0000 - val_loss: 26432744234324541440.0000 - val_mse: 26432745515874516992.0000\n",
      "Epoch 345/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 26749381378086223872.0000 - mse: 26749382874441449472.0000 - val_loss: 26517433250951446528.0000 - val_mse: 26517434299492335616.0000\n",
      "Epoch 346/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 27891879122899681280.0000 - mse: 27891872212723957760.0000 - val_loss: 28339641556770304000.0000 - val_mse: 28339639527042711552.0000\n",
      "Epoch 347/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 26420412730837790720.0000 - mse: 26420411194434125824.0000 - val_loss: 26448804716933824512.0000 - val_mse: 26448804982709813248.0000\n",
      "Epoch 348/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 27571791653397770240.0000 - mse: 27571784586669064192.0000 - val_loss: 28374036989217767424.0000 - val_mse: 28374036648806055936.0000\n",
      "Epoch 349/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 26764562640955351040.0000 - mse: 26764564930997780480.0000 - val_loss: 26890417063460995072.0000 - val_mse: 26890417230936276992.0000\n",
      "Epoch 350/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 26356386299218550784.0000 - mse: 26356382234302218240.0000 - val_loss: 28039780195285962752.0000 - val_mse: 28039780715915640832.0000\n",
      "Epoch 351/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 25618268420840321024.0000 - mse: 25618266884436656128.0000 - val_loss: 28169786668600217600.0000 - val_mse: 28169786970783875072.0000\n",
      "Epoch 352/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 26236224925834747904.0000 - mse: 26236223205572345856.0000 - val_loss: 25845100025226747904.0000 - val_mse: 25845100531293356032.0000\n",
      "Epoch 353/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 27220257710808662016.0000 - mse: 27220250928059777024.0000 - val_loss: 26434611398029152256.0000 - val_mse: 26434612486618480640.0000\n",
      "Epoch 354/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 27224811014186831872.0000 - mse: 27224809503268536320.0000 - val_loss: 27072116688213454848.0000 - val_mse: 27072115925472772096.0000\n",
      "Epoch 355/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 26642650832317440000.0000 - mse: 26642653280733233152.0000 - val_loss: 26390885622772158464.0000 - val_mse: 26390884909181829120.0000\n",
      "Epoch 356/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 26435647680456798208.0000 - mse: 26435646027548590080.0000 - val_loss: 26831081172110831616.0000 - val_mse: 26831083185454972928.0000\n",
      "Epoch 357/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 26121768474113892352.0000 - mse: 26121766244144119808.0000 - val_loss: 26237506721563258880.0000 - val_mse: 26237505236130332672.0000\n",
      "Epoch 358/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 25714042770130350080.0000 - mse: 25714047541355479040.0000 - val_loss: 27244484765062512640.0000 - val_mse: 27244484164335960064.0000\n",
      "Epoch 359/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 26803549765028020224.0000 - mse: 26803551414295461888.0000 - val_loss: 26001088690099507200.0000 - val_mse: 26001088245925937152.0000\n",
      "Epoch 360/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 27454652123888394240.0000 - mse: 27454655811985342464.0000 - val_loss: 26419576388410351616.0000 - val_mse: 26419577764620271616.0000\n",
      "Epoch 361/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 25774574058167844864.0000 - mse: 25774575656464547840.0000 - val_loss: 25765609630643576832.0000 - val_mse: 25765605840605151232.0000\n",
      "Epoch 362/500\n",
      "6040/6040 [==============================] - 1s 84us/step - loss: 27519069204343373824.0000 - mse: 27519078397279993856.0000 - val_loss: 26551107292727697408.0000 - val_mse: 26551105743581347840.0000\n",
      "Epoch 363/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 26476864333771538432.0000 - mse: 26476864519450656768.0000 - val_loss: 26800606918515511296.0000 - val_mse: 26800604723133022208.0000\n",
      "Epoch 364/500\n",
      "6040/6040 [==============================] - 1s 90us/step - loss: 26810006301189267456.0000 - mse: 26810003348527251456.0000 - val_loss: 28614365992643436544.0000 - val_mse: 28614365701382078464.0000\n",
      "Epoch 365/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 26546758598633381888.0000 - mse: 26546758274605121536.0000 - val_loss: 25893093347343237120.0000 - val_mse: 25893092014822522880.0000\n",
      "Epoch 366/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 25120338142139047936.0000 - mse: 25120339849658761216.0000 - val_loss: 26249606384649527296.0000 - val_mse: 26249606461105635328.0000\n",
      "Epoch 367/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 25953902402236272640.0000 - mse: 25953901604908302336.0000 - val_loss: 26341807088140201984.0000 - val_mse: 26341804909141164032.0000\n",
      "Epoch 368/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 25130273274596126720.0000 - mse: 25130270638680834048.0000 - val_loss: 26043360324821102592.0000 - val_mse: 26043362268990668800.0000\n",
      "Epoch 369/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 24877512950595821568.0000 - mse: 24877512706664431616.0000 - val_loss: 25871216109631688704.0000 - val_mse: 25871213932453036032.0000\n",
      "Epoch 370/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 25913887447776497664.0000 - mse: 25913890376773533696.0000 - val_loss: 25756654734647263232.0000 - val_mse: 25756655815955054592.0000\n",
      "Epoch 371/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 25073735372742283264.0000 - mse: 25073733750780592128.0000 - val_loss: 26166482007656235008.0000 - val_mse: 26166481183022514176.0000\n",
      "Epoch 372/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 26627004368627134464.0000 - mse: 26627005031246725120.0000 - val_loss: 25973335549385351168.0000 - val_mse: 25973336572440870912.0000\n",
      "Epoch 373/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 25793175788847210496.0000 - mse: 25793177194183262208.0000 - val_loss: 26157714851449978880.0000 - val_mse: 26157718075349139456.0000\n",
      "Epoch 374/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 26493501080619089920.0000 - mse: 26493495732332396544.0000 - val_loss: 25548497501522972672.0000 - val_mse: 25548496274584502272.0000\n",
      "Epoch 375/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 26238314742464368640.0000 - mse: 26238314476688375808.0000 - val_loss: 25349202838185172992.0000 - val_mse: 25349205394003591168.0000\n",
      "Epoch 376/500\n",
      "6040/6040 [==============================] - 1s 85us/step - loss: 27003256483091017728.0000 - mse: 27003257910271672320.0000 - val_loss: 25162332364979879936.0000 - val_mse: 25162332397746782208.0000\n",
      "Epoch 377/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 25984152448299081728.0000 - mse: 25984153567834931200.0000 - val_loss: 25933953675152941056.0000 - val_mse: 25933954264957190144.0000\n",
      "Epoch 378/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 25735632261644070912.0000 - mse: 25735628755585466368.0000 - val_loss: 25781500943194779648.0000 - val_mse: 25781498181673025536.0000\n",
      "Epoch 379/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 25309686605088153600.0000 - mse: 25309691145124577280.0000 - val_loss: 25204623874648412160.0000 - val_mse: 25204624012997558272.0000\n",
      "Epoch 380/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 26105275066082705408.0000 - mse: 26105275768750735360.0000 - val_loss: 25091753373790691328.0000 - val_mse: 25091748149290074112.0000\n",
      "Epoch 381/500\n",
      "6040/6040 [==============================] - 1s 90us/step - loss: 25126837741292130304.0000 - mse: 25126837963378917376.0000 - val_loss: 25225096773876068352.0000 - val_mse: 25225094720483491840.0000\n",
      "Epoch 382/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 25159480759728640000.0000 - mse: 25159475866537820160.0000 - val_loss: 24873644843203936256.0000 - val_mse: 24873644624757915648.0000\n",
      "Epoch 383/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 25339153305548066816.0000 - mse: 25339158056748974080.0000 - val_loss: 25658834210601390080.0000 - val_mse: 25658834465455079424.0000\n",
      "Epoch 384/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 24904055940414464000.0000 - mse: 24904054917358944256.0000 - val_loss: 25169371951769047040.0000 - val_mse: 25169371471187804160.0000\n",
      "Epoch 385/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 24383194849751228416.0000 - mse: 24383198667095408640.0000 - val_loss: 27570256332860645376.0000 - val_mse: 27570256265506455552.0000\n",
      "Epoch 386/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 24926417123435978752.0000 - mse: 24926412386798141440.0000 - val_loss: 24757587411533869056.0000 - val_mse: 24757586774399647744.0000\n",
      "Epoch 387/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 24748210284868653056.0000 - mse: 24748212338261229568.0000 - val_loss: 24657603540108079104.0000 - val_mse: 24657603784039464960.0000\n",
      "Epoch 388/500\n",
      "6040/6040 [==============================] - 1s 89us/step - loss: 24302104252522594304.0000 - mse: 24302100888453906432.0000 - val_loss: 26566487556279152640.0000 - val_mse: 26566485712230678528.0000\n",
      "Epoch 389/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 25990185251975053312.0000 - mse: 25990183289601654784.0000 - val_loss: 25519534031515721728.0000 - val_mse: 25519532939285626880.0000\n",
      "Epoch 390/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 24742993677436039168.0000 - mse: 24742996255099060224.0000 - val_loss: 24995960571273224192.0000 - val_mse: 24995958696278228992.0000\n",
      "Epoch 391/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 24945820936925507584.0000 - mse: 24945818767028387840.0000 - val_loss: 24857242758351273984.0000 - val_mse: 24857242110294753280.0000\n",
      "Epoch 392/500\n",
      "6040/6040 [==============================] - 1s 88us/step - loss: 24620210278285176832.0000 - mse: 24620209393578803200.0000 - val_loss: 26312826555678498816.0000 - val_mse: 26312826180679499776.0000\n",
      "Epoch 393/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 24790608580053078016.0000 - mse: 24790602909558505472.0000 - val_loss: 25441624210382848000.0000 - val_mse: 25441623744364675072.0000\n",
      "Epoch 394/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 24558050418745917440.0000 - mse: 24558049603214114816.0000 - val_loss: 25450119554207956992.0000 - val_mse: 25450120770224128000.0000\n",
      "Epoch 395/500\n",
      "6040/6040 [==============================] - 1s 86us/step - loss: 25647547386971320320.0000 - mse: 25647542481037819904.0000 - val_loss: 24887459256566759424.0000 - val_mse: 24887458888849293312.0000\n",
      "Epoch 396/500\n",
      "6040/6040 [==============================] - 1s 87us/step - loss: 25942296221449076736.0000 - mse: 25942297359188754432.0000 - val_loss: 27489737840293998592.0000 - val_mse: 27489736829981163520.0000\n",
      "Epoch 397/500\n",
      "5500/6040 [==========================>...] - ETA: 0s - loss: 26127158209184493568.0000 - mse: 26127158249166733312.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-aa3ab1d524f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    208\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m                                          \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m                                          verbose=0)\n\u001b[0m\u001b[0;32m    211\u001b[0m                     \u001b[0mval_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                     \u001b[1;31m# Same labels assumed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \"\"\"\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "model.fit(x_train, y_train, epochs=500, batch_size=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(7550,5,1)\n",
    "x_test = x_test.reshape(1888,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 16)                96        \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,681\n",
      "Trainable params: 5,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(160, input_shape=(5,)))\n",
    "model.add(Dense(160))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6040 samples, validate on 1510 samples\n",
      "Epoch 1/50\n",
      "6040/6040 [==============================] - 9s 1ms/step - loss: 389524191217854513152.0000 - mse: 389523966780017475584.0000 - val_loss: 385864514416192389120.0000 - val_mse: 385864651345290592256.0000\n",
      "Epoch 2/50\n",
      "6040/6040 [==============================] - 9s 1ms/step - loss: 385136523063873503232.0000 - mse: 385136299658679681024.0000 - val_loss: 380919034811891974144.0000 - val_mse: 380918924898251833344.0000\n",
      "Epoch 3/50\n",
      "6040/6040 [==============================] - 9s 1ms/step - loss: 377712754277390483456.0000 - mse: 377712608254168662016.0000 - val_loss: 371906013596131655680.0000 - val_mse: 371906096143976628224.0000\n",
      "Epoch 4/50\n",
      "6040/6040 [==============================] - 9s 1ms/step - loss: 364756766988976455680.0000 - mse: 364756174338688811008.0000 - val_loss: 359675310217679077376.0000 - val_mse: 359675445455945203712.0000\n",
      "Epoch 5/50\n",
      "6040/6040 [==============================] - 9s 1ms/step - loss: 353232261487315255296.0000 - mse: 353232588792154554368.0000 - val_loss: 350555572828409757696.0000 - val_mse: 350555621026147860480.0000\n",
      "Epoch 6/50\n",
      "6040/6040 [==============================] - 9s 1ms/step - loss: 343882862184276754432.0000 - mse: 343882975228245049344.0000 - val_loss: 341616909560680349696.0000 - val_mse: 341616785006375469056.0000\n",
      "Epoch 7/50\n",
      "6040/6040 [==============================] - 9s 1ms/step - loss: 335141414338634252288.0000 - mse: 335141277245286383616.0000 - val_loss: 333422468459475697664.0000 - val_mse: 333422274378142318592.0000\n",
      "Epoch 8/50\n",
      "6040/6040 [==============================] - 8s 1ms/step - loss: 325003105143883169792.0000 - mse: 325002759690401087488.0000 - val_loss: 325742230776628117504.0000 - val_mse: 325742088901103714304.0000\n",
      "Epoch 9/50\n",
      "6040/6040 [==============================] - 8s 1ms/step - loss: 318533856657142906880.0000 - mse: 318533725853776347136.0000 - val_loss: 318922639927473668096.0000 - val_mse: 318922653902846296064.0000\n",
      "Epoch 10/50\n",
      "6040/6040 [==============================] - 8s 1ms/step - loss: 311842782967716315136.0000 - mse: 311842186048061833216.0000 - val_loss: 313162420392460156928.0000 - val_mse: 313162338873206898688.0000\n",
      "Epoch 11/50\n",
      "6040/6040 [==============================] - 9s 1ms/step - loss: 306217409992636825600.0000 - mse: 306217190113476083712.0000 - val_loss: 310159092621230211072.0000 - val_mse: 310159247162308820992.0000\n",
      "Epoch 12/50\n",
      "6040/6040 [==============================] - 8s 1ms/step - loss: 302969822488934023168.0000 - mse: 302969391094700179456.0000 - val_loss: 308749728997884100608.0000 - val_mse: 308749726032058122240.0000\n",
      "Epoch 13/50\n",
      "6040/6040 [==============================] - 9s 1ms/step - loss: 303059395554064400384.0000 - mse: 303059146427898789888.0000 - val_loss: 308084685369957679104.0000 - val_mse: 308084706215207108608.0000\n",
      "Epoch 14/50\n",
      "6040/6040 [==============================] - 8s 1ms/step - loss: 300958486352346152960.0000 - mse: 300958357939218808832.0000 - val_loss: 307611635819705139200.0000 - val_mse: 307611652332472762368.0000\n",
      "Epoch 15/50\n",
      "6040/6040 [==============================] - 8s 1ms/step - loss: 302851614135043096576.0000 - mse: 302851769738807214080.0000 - val_loss: 307159430597300846592.0000 - val_mse: 307159357229270827008.0000\n",
      "Epoch 16/50\n",
      "6040/6040 [==============================] - 8s 1ms/step - loss: 300333833259459018752.0000 - mse: 300333870519014129664.0000 - val_loss: 306744157335551934464.0000 - val_mse: 306744111269878431744.0000\n",
      "Epoch 17/50\n",
      "6040/6040 [==============================] - 9s 1ms/step - loss: 299129534652576104448.0000 - mse: 299129368724925054976.0000 - val_loss: 306331880129249345536.0000 - val_mse: 306331996719601942528.0000\n",
      "Epoch 18/50\n",
      "6040/6040 [==============================] - 8s 1ms/step - loss: 300236094357023293440.0000 - mse: 300236022780235087872.0000 - val_loss: 305899069575331512320.0000 - val_mse: 305899228942909308928.0000\n",
      "Epoch 19/50\n",
      "6040/6040 [==============================] - 9s 1ms/step - loss: 301869532489256140800.0000 - mse: 301869457254459113472.0000 - val_loss: 305504159977406595072.0000 - val_mse: 305504178813095903232.0000\n",
      "Epoch 20/50\n",
      "6040/6040 [==============================] - 9s 1ms/step - loss: 301424428091462320128.0000 - mse: 301424797160000454656.0000 - val_loss: 305101241599225823232.0000 - val_mse: 305101317752678776832.0000\n",
      "Epoch 21/50\n",
      "6040/6040 [==============================] - 9s 2ms/step - loss: 301290891797064122368.0000 - mse: 301290709517969915904.0000 - val_loss: 304733898870914351104.0000 - val_mse: 304733957723699281920.0000\n",
      "Epoch 22/50\n",
      " 654/6040 [==>...........................] - ETA: 7s - loss: 236283470478925365248.0000 - mse: 236283623841992802304.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-7ebb52bbec93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \"\"\"\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
